[["index.html", "Extending Data Curation to Interdisciplinary and Highly Collaborative Research (IHCR) Chapter 1 About", " Extending Data Curation to Interdisciplinary and Highly Collaborative Research (IHCR) 2023-05-23 Chapter 1 About "],["our-team.html", "Chapter 2 Our Team", " Chapter 2 Our Team "],["practical-recommendations-and-workflows.html", "Chapter 3 Practical recommendations and workflows", " Chapter 3 Practical recommendations and workflows "],["other-outputs.html", "Chapter 4 Other outputs", " Chapter 4 Other outputs "],["extending-data-curation-to-interdisciplinary-and-highly-collaborative-research.html", "Chapter 5 Extending data curation to interdisciplinary and highly collaborative research", " Chapter 5 Extending data curation to interdisciplinary and highly collaborative research Interdisciplinary and highly collaborative research (IHCR), defined as research that integrates resources and expertise across disciplines and insitutional settings, addresses complex problems and seeks solutions to societal challenges. Such research, however, places higher demands on communication, learning, and trust, all of which can have implications for how the data is collected, managed, and preserved. In this repository we collect workflows and recommendations that can help IHCR teams to improve their data management. "],["workflow-creating-and-maintaining-a-catalog-of-data-assets.html", "Chapter 6 Workflow: Creating and Maintaining a Catalog of Data Assets 6.1 Introduction 6.2 Workflow Diagram 6.3 Implementation Steps 6.4 Appendices: Detailed Instructions 6.5 Best practice recommendations for Tabular Data Processing:", " Chapter 6 Workflow: Creating and Maintaining a Catalog of Data Assets 6.1 Introduction Keeping track of information assets is a significant challenge in projects that conduct interdisciplinary and highly collaborative research (IHCR). In such projects, data and information may be spread across multiple storage systems and institutions; multiple contributors may be responsible for it at various stages of the project lifecycle. Commercial products such as Starfish1 provide cataloging and tracking functionality, but such products may be costly and have technical barriers for integration and maintenance. The workflow below provides a low-cost way for IHCR teams to create and maintain a catalog of their data assets. When used in combination with the overall project plan to guide teamwork and data management (see Project Charter), it will improve consistency and efficiency of IHCR data practices. While this workflow focuses on data, it can also be used for managing digital information assets more broadly. 6.2 Workflow Diagram 6.3 Implementation Steps 6.3.1 Assign Roles and Responsibilities The first step in tracking data assets is to designate the roles and responsibilities of project participants to decide who will be involved in data collection and management and how. Identify who will serve as the designated data manager and who will assume their responsibilities in the event the data manager is not available. Depending on the number of project participants, the responsibilities of the data manager can be assigned to one or more individuals. 6.3.2 Select Storage Once the project team is formed, members can review and decide on the data storage location and the directory structure for all project files and data assets. The choice of storage platform will also impact which of the data catalog options are available; solutions offered below are for local file storage, Google Drive, and the Microsoft 365 cloud suite (OneDrive and Sharepoint). See also: Information Organization 6.3.3 Establish Metadata Procedures The data manager (and other members of the team) will need to determine what information about data assets (metadata) is most helpful for data use and reuse. As a general rule, metadata is the who, what, when, where, why, and how of the research data. A basic set of metadata fields is described below in the section “Manual Creation of Data Catalog”, but there will likely be additional fields specific to the project that should be included for the most complete description of each data asset. 6.3.4 Setup and Create Data Catalog The data catalog can be set up and created via manual collection that uses a data cataloging form, automated inventory procedures (see “Appendices” for detailed instructions), or a combination of both methods. If using both the manual and automated collection methods, the data manager will need to reconcile the data catalog with the periodic inventories to maintain a full record of the project’s data assets. As part of the catalog setup, all project members need to be assigned access permissions. For example, the catalog should be viewable to any project member, but it should only be edited by the data manager(s) and data cataloger(s). As part of the catalog setup, all project members need to be assigned access permissions. For example, the catalog should be viewable to any project member, but it should only be edited by the data manager(s) and data cataloger(s). 6.3.5 Review and Update Regularly The data manager should periodically review the data catalog to ensure consistency and check that no information is missing. This includes confirming that file naming conventions are followed, that files are stored in the correct locations, and that there is no unnecessary duplication of files. If additional metadata fields were not added to the catalog by the workflows described in this document, they need to be added to the catalog manually. Periodic evaluation and update can be done in two ways: via a manual review of the data catalog or by running the automated inventory described in Appendix B. Reviews should be done at regular intervals to avoid a backlog of work and to catch and rectify potential errors in cataloging as early as possible instead of at the end of a project’s data collection. The reviews should be scheduled in weekly or monthly intervals depending on the pace of the project and its data collection processes. 6.4 Appendices: Detailed Instructions 6.4.1 A. Manual Creation of Data Catalog The data catalog is a list of all data assets that are described consistently and uniformly using a set of attributes. Some attributes may be set to “optional” if they are not essential to each data asset. The catalog can be created and tracked manually via a data cataloging form using Google forms, Microsoft forms, Qualtrics, or any other surveying or spreadsheet software. It can also be created as a template spreadsheet where each attribute is a column and each data asset forms a row. The template below lists the suggested minimum information needed to describe each asset; it can be customized to add more metadata fields according to the needs of a project. Asset ID (required): Unique asset ID assigned automatically or by the data manager. This field needs to be entered directly into the catalog instead of through the cataloging form. Data Collector (required): Name(s) of individual(s) who collected the data. Contributor (if different from data collector, optional): Name of the individual who created the record. Date of Collection (required): Date(s) data was collected. Keyword(s): A list of project-specific descriptors that can help in search and organization. These tags can include experiment names, institutions, geographic locations, and so on. At the beginning the tags can be free-form, but later on it is recommended to standardize the tags and create a controlled vocabulary. Description (required): Short description of contents (what is the nature of this data, methods used in collection, any details of importance to the project). State (optional): Current state of the asset (e.g., to be created, created, cleaned, transformed, verified). The states are project-specific and can be determined by the team in advance. Storage Location (optional): Location of the asset being cataloged (e.g., central project location, individual member’s storage, etc.). Path (optional): Where or how to find this asset. Can be a full path (e.g., “C”:My DriveFilename.csv”) or a relative path (e.g., folder “Data files”). Notes (optional): Any notes about the item being cataloged that are useful to include in the catalog. 6.4.2 B. Data Catalog via Automatic Inventory 6.4.2.1 B1. Use Microsoft Excel to Create a Data Catalog of a Local Directory (Windows Machine Only) The following steps will create a file list of a specified directory (and any subdirectories) stored on a local machine. The file list will include file information metadata and the link to both the individual file and to the folder a file appears in. Open a new workbook in Excel, then navigate to “Data”-&gt; “Get Data”-&gt; “From File” -&gt;“From Folder” Choose the directory you want to index and click “OK” (older versions of Excel) or “Open” (newer versions). In the new dialogue box that opens, choose “Transform Data”. In the Power Query window that opens, the fields that will be created for each file are listed as columns. To remove a field before running the query, right click the heading title and choose “Remove”. Click the “Close and Load” button at the top left corner on the menu; the data will then populate in the sheet. Save the file according to the file naming convention determined by the data manager to save a static copy of the directory’s contents. To have snapshots of the catalog over time, create an inventory periodically and add a date to the file name. 6.4.2.2 B2. Create a Data Catalog of Microsoft Online Document Storage (SharePoint) and Save It on a Local Machine (Windows Machine Only) Note: This workflow generates a web query file “query.iqy” that cannot be run or opened on a Mac as the internet query connection is not supported by Excel for Mac. As a workaround, you can run this on Excel for Windows, save the file as XLSX and then open it in Excel for Mac. You’ll be able to see the data but you won’t be able to refresh the data connection. Open SharePoint in a browser and log in as necessary. Navigate to the directory that needs to be indexed. Note: Organizations may have their SharePoint platforms configured differently. Navigate to “Documents” within the project’s Sharepoint page and then open the directory to be indexed. Click “Export to Excel”. This will generate and download an Internet Query File called “query.iqy” (the file should download into the default download folder for your browser). Navigate to your download folder and double-click on the ‘query.iqy’ file. Enable a connection to your online SharePoint platform when prompted (see image below). The file will be opened as an empty workbook initially. You will be asked to select how the data should be imported. Select “Table” to have the full folder contents listed (selecting “PivotTable Report” or “PivotChart” will aggregate and summarize the data rather than generate full folder contents). Next, under “Where do you want to put the data?”, select the location for the index (in a designated area of the existing worksheet, a new worksheet in the current file, or in a new workbook). The new file will be populated with the following information: Name: name of the file or folder with a link to its online version Modified: the date when the item was modified Modified by: name of the user who modified the item last Item type: “item” refers to files, “folder” refers to folders Path: the path within folder hierarchy (no link to the online content). Save the file according to the file naming convention determined by the data manager to save a static copy of the directory’s contents. To run the index again on the same directory, either run the downloaded iquery file again or follow the steps above to download a new iquery file. If you plan to use the same iquery file to index a directory each time, it is recommended that you rename the file with the name of the Sharepoint Site and Directory it indexes. By default, the data catalog generated by this query is arranged alphabetically by name with all folders going first and all items (files) going after all the folders. To view the index by directory, sort the rows alphabetically by the column “Path” (Home -&gt; Sort &amp; Filter - Custom Sort). To sort further within each folder, use “Custom Sort” and add columns to the sort parameters (e.g., name or modified). To view files only (without the folder names in the column “Name”), click on the down arrow to the right of the column “Item Type” to view options for that field, and then deselect “Folder”. To only display items from specific folder(s) for review, click on the down arrow to the right of the Path column, uncheck “Select All”, and then select only the desired folder(s): 6.4.3 B3. Create a Data Catalog in Google Drive using a Google App Script The following steps will create a recursive file listing of a specified directory (and any subdirectories) stored in Google Drive with metadata, the file path, and links to both the individual file and to the folder a file appears in. Create a new google sheet to store the file inventory in your project directory. Add the App Script to the document: Go to “Extensions”-&gt; “Apps Script”. In the App Script that opens, remove any content that is there and paste the code block from the end of this section (if needed, the metadata fields to be collected can be changed and/or reordered in the last section of code). Give the script a meaningful name (e.g., “FileList”), then save the project by clicking on the diskette icon at the top. Click “Run”. Close the tab with the Apps Script and the tab with the google sheet, then reopen the sheet. You will now see a new menu option named “File List”. To run the inventory, click “File List” then choose “Create a file list from folder”. Enter the name of the directory whose contents you want to inventory in the dialogue box that opens. If asked to allow access, choose “Continue” then provide your credentials. (You may need to repeat the previous step after providing authorization). The script will then populate the sheet with a list of files from the specified directory with the metadata specified. You can run the script in multiple worksheets within the same GoogleSheet for different directories if needed, or run it for the full directory. Code for Apps Script //Function that creates the menu link in the sheet to run the inventory function onOpen() { var ss = SpreadsheetApp.getActiveSpreadsheet(); var searchMenuEntries = [{ name: &quot;Create a File List from Folder&quot;, functionName: &quot;start&quot; }]; ss.addMenu(&quot;File List&quot;, searchMenuEntries); } //Function that builds the inventory function start() { var sheet = SpreadsheetApp.getActiveSheet(); sheet.clear(); //Comment below enable it if you need one sheet //Builds the column headings sheet.appendRow([&quot;Name&quot;, &quot;Folder&quot;, &quot;Type&quot;, &quot;Creation Date&quot;, &quot;Size&quot;, &quot;URL&quot;,&quot;Folder URL&quot;, &quot;Path&quot; ]); //Makes the Input Box to specify which folder the inventory will run on var folderName = Browser.inputBox(&quot;Enter the Name of the Directory to be Inventoried (this will overwrite the current spreadsheet):&quot;); //Fetches the folders var folder = DriveApp.getFoldersByName(folderName); if (folder.hasNext()) { processFolder(folder, &#39;&#39;); } else { Browser.msgBox(&#39;Folder not found!&#39;); } //Returns the information for each file in the specified folder function processFolder(folder, path) { while (folder.hasNext()) { var f = folder.next(); var contents = f.getFiles(); var fName = f.getName(); addFilesToSheet(contents, f, path); var subFolder = f.getFolders(); processFolder(subFolder, path + &#39;/&#39; + fName); } } //Outputs the file inventory function addFilesToSheet(files, folder, path) { var data; var folderName = folder.getName(); while (files.hasNext()) { var file = files.next(); Logger.log(file.getName()); Logger.log(file.getParents()); //Specifies the fields returned in the inventory var fileName = file.getName(); sheet.appendRow([ fileName, folderName, file.getMimeType(), file.getDateCreated(), file.getSize()/1024, file.getUrl(), folder.getUrl(), path + &quot;/&quot; + folderName + &quot;/&quot; + fileName, ]); } } } 6.4.4 B4. Create a Data Catalog of Files Stored in a Shared Linux File Mounting System The following steps will create a catalog of files stored in a shared project space in a Linux File Mounting System, such as Lustre, using Terminal. Using Linux terminal, connect to the file system where your project data is stored (for this, you will need to be connected via ssh, not sftp). Navigate to the directory where your data is stored and create a directory named ‘Data Catalog’ to store catalog files. Run the following command, replacing ‘filename’ with the name you want for the Data Catalog and ‘date’ with the current date. Find “$PWD” -type f | xargs ls -l &gt; ~/DataCatalog/filename_date.csv This will create a .csv file with a list of each file with its path and its file size and save it in your home directory on the file system in the directory you’ve created named ‘DataCatalog’, 6.5 Best practice recommendations for Tabular Data Processing: Order Best Practice Implementation 1 Do not change any original data Mark all original data as “read-only” 2 Assign unique ordinals to each row A new right most column 3 For each derived/processed column add a new column A new right most column 4 Use visual cues for types and provenance of data Different column heading highlights for read-only, derived data and groups of columns 5 Identify key fields in the data Identify identifiers 6 Mark all missing data in new columns Explicitly state what the “N/A” data constants are and use “token” that is not valid data in any field. 7 Identify duplicates Explicitly state how duplicates are dealt with 8 Identify inconsistent data (controlled vocabulary inconsistencies) Construct lookup tables for many to one mappings 9 Standardize values: units Localization in office documents 10 Standardize values: dates Localization in office documents 11 Standardize values: names People, places, controlled vocabulary See the Roadmap section of Budak et al. (2020) for a good example of an IHCR project timeline↩︎ "],["preservation-workflow-for-platform-data.html", "Chapter 7 Preservation Workflow for Platform Data 7.1 Introduction 7.2 Workflow Diagram 7.3 Detailed Instructions 7.4 Additional Notes/Guidelines 7.5 References", " Chapter 7 Preservation Workflow for Platform Data Date Created: January 3, 2021 7.1 Introduction Workflow for preserving data in a digital project with the aim of preserving the data for the project as an archive that: - preserves the dataset and results of the project; - contains all necessary materials to redeploy or recreate the project; - allows replication/confirmation of the results of analysis done within the project. 7.2 Workflow Diagram Create a Datamap: outline the data structure for the entire data archive for the project List each subset of data with their main components Original Data Original data for project Source material All open source file types, if possible, or at least very common ones Code written for data extraction/cleaning/structuring, if used Platform Data Dataset as structured for ingest into a platform itself and/or resources that appear in the site (maps, word clouds, charts, etc.)(excel sheets, .csv, etc.) Surrogates for dynamic/visual resources Code for custom tools/viz built into the site Site Files Server files Database files (if applicable) static HTML pages from the finished project Documentation Documentation for dataset curation process Documentation for building site and other web resources Documentation for analytical tools Design a Data lifecycle management plan Consult with Research Data Librarian about repositories requirements How long will data be stored? What maintenance will be required to keep data usable over time? Where will the dataset be hosted? (recommend institutional repositories) Who will it be accessible to? Plan for rights for different parts of the dataset 7.3 Detailed Instructions Creating a Datamap: outline of the data structure for the entire data archive for the project List each subset of data (original data, platform data, site files, dynamic/visual surrogates, and documentation) with main components Original Data Original data for project (best case: in tabular form, with metadata, including a list of all source materials with their metadata) Source material- copies of annotated documents, etc., used for data extraction (if sources have been OCR’d, annotated or marked up in TEI, transcriptions of sound/video recordings in addition to the media files, etc.) All open source file types, if possible, or at least very common ones Code written for data extraction/cleaning/structuring, if used Platform Data Dataset as structured for ingest into a platform itself and/or resources that appear in the site (maps, word clouds, charts, etc.)(excel sheets, .csv, etc.) Likely comprises a set of versions from ingests taking place over life of project- could include as versions, or best case as one set for the final ‘complete’ project Data downloaded and/or exported from platform Includes (best scenario) a tabular export of all data with metadata in the site; minimum, a list of all downloaded assets with basic information (date, size, description) Multiple file formats (including likely some proprietary forms depending on the platform) Depending on the platform (like ArcGIS), not everything can be downloaded directly. Feature layers can be opened in ArcGIS pro and then saved as part of a project file, but will still be in a proprietary format. Surrogates for dynamic/visual resources Videos of dynamic resources, stills of maps and other visualizations as image files and/or pdfs, etc. Code for custom tools/viz built into the site Site Files Server files (checking for hotlinked resources) Database files (if gets to this stage) static HTML pages from the finished project Documentation Documentation for dataset curation process, including transcription guidelines (if applicable) Documentation for building site (including data ingest) and other web resources Documentation for analytical tools (tools and settings used, etc.) Design a Data lifecycle management plan As early as possible, consult with Research Data Librarian about repositories requirements (file types, other recommendations) How long will data be stored? What maintenance will be required to keep data usable over time? Where will the dataset be hosted? (recommend institutional repositories) Who will it be accessible to? Depending on the repository, embargos may be possible Plan for rights for different parts of the dataset 7.4 Additional Notes/Guidelines Opportunities for automation in this process? - File conversion (from excel to .csv, word to .txt or pdf, etc.) - Generating list of files/file sizes of archives 7.5 References The Endings project https://endings.uvic.ca/about.html Visualizing Objects, Places, and Spaces: A Digital Project Handbook Example: Urban Green workflow demo: https://docs.google.com/spreadsheets/d/1D-749mVv6bR2DynD57xJmg3bHIftYddiPnreBxKYTH8/edit?usp=sharing "],["organizing-information-and-data-for-interdisciplinary-projects.html", "Chapter 8 Organizing Information and Data for Interdisciplinary Projects 8.1 Introduction 8.2 Detailed Description of Steps 8.3 Resources and Examples", " Chapter 8 Organizing Information and Data for Interdisciplinary Projects 8.1 Introduction Efficient information storage and organization is integral to interdisciplinary and highly collaborative research (IHCR). Even though working with IHCR data is in many ways similar to working with data that comes from disciplinary projects, there are some important differences: * Many IHCR projects are grant-funded, so the need for efficient long-term management of data is more pressing considering that the funding may not be renewed. * IHCR projects are characterized by data and methodological heterogeneity, i.e., the data collection and analysis techniques come from various domains that have varying rules and procedures. * IHCR projects often use online platforms in addition to traditional scholarship venues (papers, conferences) to disseminate their work. * IHCR projects are often “cross-institutional,” which makes storage and organization even more complex. Due to their shorter and more heterogeneous nature, IHCR projects require more coordination. A consistent approach to information storage and organization can help reduce the burden of coordination and allow the project to track and update resources as needed. The workflow below provides a set of recommended steps for organizing data and information. ## Workflow Diagram 8.2 Detailed Description of Steps 8.2.1 1. Set goals The goals of data and information organization (which in most practical cases is synonymous with file organization) need to be aligned with the goals of the project. For research projects, the goals usually include collecting and analyzing data and delivering results in the form of papers and presentations. For IHCR projects, the goals also need to address their heterogeneous nature. In addition to the project goals, goal-setting for file organization includes understanding who will be using the file system. 8.2.2 2. Determine storage needs Determining storage needs includes determining the amount of storage that will be needed for the project, modes and rules of access, existing storage options, and the alignment between storage and computation. To determine project storage needs, consider the following: * With whom the data will be shared regularly (e.g., collaborators at your institution, external collaborators)? * What is the classification of your data (e.g., public, sensitive, confidential/restricted, HIPAA-compliant)? Many universities have classifications of their institutional data, which can be used as a guideline. * What are your data safety procedures? (e.g., number of backup copies, backup locations, frequency of snapshots). * How much data do you have and how fast will it grow? (e.g., under 1TB/year, over 1TB/year). * Do you have special performance needs? (e.g., thousands of files, frequent transfer, large computational needs). * How are you expecting to access the data? (e.g., from own computer, remotely from any computer). * Who is responsible for the data storage and its maintenance? (e.g., yourself, another team member, your institution). * What is your budget for data storage and management and where does it come from? (e.g., no budget, fixed amount from grant, ongoing support from institution). 8.2.3 3. Evaluate options and select the appropriate storage To evaluate storage options, the team may want to consult with institutional IT, the library, or a dedicated data management group at their respective institutions. 8.2.4 4. Organize files into folders and sub-folders Folder names and structure are highly dependent on the nature of the project. No files should be stored outside of folders, and each file is related to a category within an area. Start with a minimal hierarchy and add complexity only when certain that it will simplify daily work. Below is a suggested structure that can be adapted to many research projects: Administration Hiring Finances .. Grant materials Proposal Reports .. Instruments Questionnaires .. Data Collected Processed Analysis Publications and presentations Archive 8.2.5 5. Establish a file naming convention Define your file naming convention based on aspects that are important to the project. Use descriptive names that include a) a date, b) experiment conditions, c) location, d) researcher name/initials, e) version number. Decide whether to use underscore (_), hyphen (-), or camelCase consistently in file names between the aspects chosen. Include dates in the YYYYMMDD or YYYY-MM-DD format at the beginning of the name to keep files in chronological order date_content 8.2.6 6. Consider using tags and controlled vocabularies File names may be more useful if standardized information is added to them. For example, the filenames may include designations for sharing status, readiness, and so on. Data: [raw] [processed] [reshaped] [cleaned] Sharing: [noshare] [shared] [public] Readiness: [first draft] [revision] [final] Funding: [submitted] [rejected] [awarded] 8.2.7 7. Establish a system for version control Most commonly used method includes a combination of date, sequential numbering and researcher initials for version tracking. This method is prone to errors and requires a lot of effort in tagging / renaming each file. A better way for version control is to use the established version control systems such as Git, CVS, or SVN, however, they require investments in learning, training, and adoption. 8.2.8 8. Define access permissions Determine who on the team should be able to access and/or edit which files. Implement access controls as needed to ensure file integrity and/or comply with requirements for data that has been classified as sensitive, confidential, HIPAA-protected, etc. 8.2.9 9. Solicit input from team members Once the system has been in place for a reasonable amount of time, reach out to the team members and solicit their input on the following: * What works well in the current system? * What is not working with the current system? * What is missing from the current system? * Discuss and review the feedback and determine what changes can improve the system without adding too much burden to maintenance and implementation. 8.2.10 10. Maintain the system Schedule recurring maintenance time to review filing practices and move misplaced files. Review the existing files and folders and delete or archive what is not needed. Designate someone on your team as the go-to person for all file-organizing questions. Check in with your team periodically to see whether your organization system is working and adjust as necessary. 8.3 Resources and Examples Illinois Library File Naming Conventions https://guides.library.illinois.edu/introdata/filenames How to Design File Management for a Company https://karl-voit.at/2021/01/11/company-file-management/ Gentzkow, M. &amp; Shapiro, J. M. (2014). Code and Data for the Social Sciences: A Practitioner’s Guide. https://web.stanford.edu/~gentzkow/research/CodeAndData.pdf Data Storage Finder at Cornell University https://finder.research.cornell.edu/ Store, share, and collaborate on institutional files at IU https://kb.iu.edu/d/bgeo File Transfer, Storage, and Infrastructure at University of Colorado Boulder https://oit.colorado.edu/services/file-transfer-storage-infrastructure File naming apps: Bulk Rename Utility (Windows), Renamer (Mac), PSRenamer "],["implementing-a-charter-for-interdisciplinary-and-collaborative-research-projects.html", "Chapter 9 Implementing a Charter for Interdisciplinary and Collaborative Research Projects 9.1 Introduction 9.2 Workflow Diagram 9.3 Detailed Description of Steps 9.4 Charter Template 9.5 Resources and Examples", " Chapter 9 Implementing a Charter for Interdisciplinary and Collaborative Research Projects 9.1 Introduction A project charter documents the project’s scope, guiding principles, objectives, and roles and responsibilities. Creating and implementing a charter helps to optimize resources for the project and ensure the high quality of data and other outputs generated in the course of the project. For interdisciplinary and highly collaborative research (IHCR) a charter helps to create a shared understanding of the project and align diverse backgrounds and expectations. It can also act as an ethical guide to ensure proper credit and attribution and to resolve conflicts. Charters are meant to be living documents that evolve with the project and the team’s needs. They need to be written collaboratively and revised regularly. Drafting a project charter can facilitate important conversations early in the project to mitigate conflict or misunderstandings. It can also be a useful tool for onboarding new team members. The charter workflow below is organized into four broad sections: Brainstorm and Clarify, Work Together, Manage Data, Plan Outputs. After going through the steps of this workflow, the project team should have a good draft of the project charter to discuss and finalize. 9.2 Workflow Diagram 9.3 Detailed Description of Steps 9.3.1 1. Brainstorm and Clarify 1.1 Get together as a team and create a concise description of your project and its vision. Clarify the “why’s” and “what’s” of the project and make sure that the team has a shared understanding of the goals and scope of the project. 1.2 Discuss assumptions and pre-existing conditions, e.g., funding and constraints, support from informal campus partners, and disciplinary differences. 1.3 Discuss project continuity and sustainability. Will the project continue beyond its initial timeline? How will you deal with turnover of project members? What happens if a team member moves institutions? Suggested activities: Create an “elevator pitch” for your project. Identify the project’s strengths, weaknesses, and constraints. Discuss best case and worst case scenarios for project results. Create a team-generated glossary of terms. 9.3.2 2. Work Together 2.1 Understand your team and what each part of the project depends on. Does the project include students, external consultants, volunteers? What are the roles of everyone on the project? 2.2 Discuss expectations and modes of communication for your team, including any project management tools you will use, frequency of meetings, and expectations for response times, including communication outside of regular meetings. 2.3 Establish the main principles that will guide the work of your team. Possible topics to cover include: Values (e.g., good faith and respect, openness and honesty) Governance (how decisions are made and how to resolve conflicts and disagreements) Recognizing diverse labor contributions Open and transparent research Diversity, equity, inclusion, &amp; access Suggested activities: Review available communication channels and resources. Discuss conflict resolution techniques. Discuss how to balance disciplinary perspectives with the project’s goals. 9.3.3 3. Manage Data 3.1 Identify the data you already have, secondary data you plan to use, and data you need to collect. 3.2 Discuss methods of data gathering and analysis, especially if methods come from different disciplines. Does the team have the necessary skills? Which tools will be needed? Does everyone in the team have access to those tools? 3.3 Identify how data will be stored, organized, and shared. Discuss backup procedures. 3.4 Establish rules for archiving or preservation of research data post-project, compliance with funding agencies, and embargo rules. Suggested activities: Create a data management plan. Develop a data catalog. Establish procedures for documenting metadata (describing your data). 9.3.4 4. Plan Outputs 4.1 Discuss the intended audiences for this research and the impact of research on various fields on team members’ careers. 4.2 Identify potential venues and forms of publications, e.g., papers, presentations, datasets, and other research outputs. Discuss how interdisciplinarity affects the choice of venues and publications. 4.3 Sketch out a timeline with major milestones and deadlines. Be sure to include any funding or publication/conference deadlines. List major tasks towards the project deliverables. These might align with the research lifecycle and could include things like: acquire equipment, train students on data collection, etc. If possible, indicate responsible team members. These should be high level tasks. You may also discuss how you will adjust for unexpected delays2. 4.4 Develop rules for authorship and attribution. IHCR projects involve many actors at different levels. Make sure that all team members get credit for their work and identify which contributions merit author recognition on papers and presentation. Other considerations include decisions regarding authorship order, creating a project website with a comprehensive list of team members, and how disagreements will be resolved (especially, in the cases of a power imbalance). Suggested activities: Select a project management approach (e.g., a tracking spreadsheet, external tools such as Trello.com. Review contributor roles taxonomy https://credit.niso.org/ and discuss how these roles apply to your project. 9.4 Charter Template Project title Project description (3-5 paragraphs) Elevator pitch (1 paragraph) Team members and their roles Project principles and values Infrastructure Communication tools Storage Analysis software Deliverables and due dates 9.5 Resources and Examples Indiana University Institute for Digital Arts &amp; Humanities Project Charter Template Center for Digital Humanities Princeton Project Charters Budak, N., Rustow, M., Koeser, R. S., Doroudian, G., Ermolaev, N., Luescher, S., &amp; Richman, R. (2020). CDH Project Charter – Princeton Geniza Project 2020-2021. Center for Digital Humanities at Princeton. https://doi.org/10.5281/zenodo.4081714 Ruecker, S., &amp; Radzikowska, M. (2008). The iterative design of a project charter for interdisciplinary research. Proceedings of the 7th ACM Conference on Designing Interactive Systems, 288–294. https://doi.org/10.1145/1394445.1394476 See the Roadmap section of Budak et al. (2020) for a good example of an IHCR project timeline↩︎ "],["processing-collected-quantitative-data-for-easy-analysis-input.html", "Chapter 10 Processing Collected Quantitative Data for Easy Analysis Input 10.1 Introduction 10.2 Workflow Diagram 10.3 Detailed Steps 10.4 References", " Chapter 10 Processing Collected Quantitative Data for Easy Analysis Input 10.1 Introduction Within interdisciplinary or highly collaborative research groups, quantitative data collected by one subgroup may require analysis by another subgroup. In seeking efficiency and internal logic, the subgroup collecting the data may collect and record the data in a manner that does not facilitate subsequent data analysis. For example, data about agricultural production is commonly collected in an order related to its location in the field, e.g. moving from one agricultural plot to an adjacent agricultural plot. While collecting agricultural production data in this way is less time consuming, it does not commonly lead to a set of data amenable for immediate data analysis. Thus processing collected quantitative data from one subgroup such that data analysis is easier for another subgroup is commonly needed. Here we present general steps (including a diagram), guidelines, and references for automating processing of collected quantitative data from one subgroup such that data analysis by another subgroup is eased. Vital to creating a successful process to transform collected data for easy analysis input is obtaining appropriate context from the data collection and data analysis subgroups. To make this process modular and extensible for future data collection, good software engineering and documentation practices are also important. 10.2 Workflow Diagram 10.3 Detailed Steps These steps are written as if the person automating the processing of collected quantitative data for analysis input is not associated with either data collection or data analysis. If they are associated with either, steps for which they already have the appropriate content or context can be skipped. Obtain from the research group a representative sample of collected data, including information on how the data was collected and why it was collected that way (i.e., its metadata). “Representative” in this case simply means that the sample of collected data and metadata is sufficient to fully understand the data collection process and the nature and structure of the data. Ask the data collection subgroup about the number of records in the collected dataset to ensure the planned algorithm is appropriate for pre-processing this size of data. Confirm that the data collection process will be similar in the future; otherwise developing an automated process may not be as beneficial. Obtain from the research group a representative sample of processed data ready for input into the analysis application, including what analysis application and input data structure will be used. By representative is meant that the sample of processed data, including metadata, is sufficient to fully understand the data processing procedure. Develop algorithm to pre-process and document preprocessing workflow for case study. Share diagram or example of algorithm with subgroup representatives for their review, editing and ultimate approval. With input from subgroup representatives, may determine that data collection process could/should change, or that preprocessing output should be altered. Could be an iterative process. Ask each subgroup representative what context is required for both sub-groups to understand how preprocessing data for analysis input is to be done (and also for the use of a wider community) Include this context in algorithm for later documentation. Recommend output of data for analysis input to be in open formats (e.g. csv). Develop script/code to execute agreed-upon algorithm. See if packages available from frictionlessdata or other already existing functions will support this pre-processing. Use programming/scripting language that is familiar to subgroup representatives. Script/program documents provenance of pre-processing workflow (possibly in a log file) for improved data re-use. Incorporate best practices of working with data (e.g., keeping raw data intact, using code commenting, assertions). Documentation for the code should include: Information on how data was collected by that particular subgroup. Context agreed upon by subgroup representatives to understand data processing. Format of data after preprocessing, and its use in data analysis by that particular subgroup. Other software documentation best practices (e.g. description of dependencies). Data Curation Network primers could be useful for specific languages/applications. Demonstrate script/code to research group members, revise as necessary. Add appropriate open-source license to the code and its documentation. Place a copy of the script, code, and documentation in a publicly accessible location for further development. One should also be placed in an archival repository. 10.4 References Software Engineering Ten simple rules for making research software more robust ESIP software guidelines Software Curation/Archiving Data Curation Network primers Software Metadata Recommended Format (SMRF) Guide "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
