[["index.html", "Extending Data Curation to Interdisciplinary and Highly Collaborative Research (IHCR) Introduction", " Extending Data Curation to Interdisciplinary and Highly Collaborative Research (IHCR) 2023-10-26 Introduction This online resource provides information and outputs from the project that focused on examining data practices in interdisciplinary and highly collaborative research (IHCR). We defined IHCR broadly as research that integrates resources and expertise across disciplines and professional and institutional settings. A collaboration between three academic institutions in the US, Indiana University Bloomington, Virginia Tech, and University of Colorado Boulder, the project was funded by the Institute of Museum and Library Services (Award #LG-246400-OLS-20). The project’s goals included developing a deeper understanding of IHCR data practices and providing insights into the tools and processes that interdisciplinary researchers employ in their work. Through methods of qualitative research that included interviews, observations, document analysis, and iterative follow-up conversations, we examined the practices of six interdisciplinary teams from three US universities. Using the knowledge derived from studying IHCR practices across various teams, we developed practical recommendations for data curation and how library and research technology units can engage with IHCR teams. These recommendations and the other outputs of the project can be found in the specific sections of this site via the left-hand navigation. Project period of performance: 2020-2023 "],["team.html", "Chapter 1 Team", " Chapter 1 Team Our project team combines technical, social science, and professional expertise. Inna Kouper, Indiana University: Project director, PhD in information science and sociology. Expertise in qualitative and quantitative research, curation of diverse datasets and collaborations in interdisciplinary teams. Theresa Quill, Indiana University Libraries: Spatial data and digital humanities expert. Experience with interdisciplinary outreach and consulting on spatial data usage and creation to users with varying levels of experience. Esen Tuna, Indiana University: Research data services manager and technical advisor. Expertise in scalable and mission critical data services architecture, development and management. Experience in research data workflows and management, and in externally funded collaborative large-scale software development projects. Dimitar Nikolov, Indiana University: Expertise in software engineering and testing, workflow development, requirements analysis, communication skills and teamwork. Katie Chapman, Indiana University: Expertise in front end engineering and testing, workflow development, requirements analysis, communication skills and teamwork, PhD in Musicology. Thea Lindquist, University of Colorado Boulder: Expertise in interdisciplinary project management, PhD in history. Experience leading a campus research center at CU Boulder that integrates and promotes research data management and curation, digital scholarship, and scholarly communication. Andrew Johnson, University of Colorado Boulder Libraries: Expertise in research data management and curation, including active curation of research data and consultations with researchers from a wide range of academic disciplines. Jonathan Petters, Virginia Polytechnic Institute and State University Libraries: Expertise in research data management, consulting and data curation; research background in computational modeling and comparison of model output to observations. Each institution also engaged students, Kimberly Cook, Urooj Raja, Truitt Elliott, and Ece Gumusel, and drew widely on the local expertise of data curators and research technologists. "],["Project-charter.html", "Chapter 2 Implementing a Charter for Interdisciplinary and Collaborative Research Projects 2.1 Introduction 2.2 Workflow Diagram 2.3 Detailed Steps 2.4 Charter Template 2.5 Resources and Examples", " Chapter 2 Implementing a Charter for Interdisciplinary and Collaborative Research Projects 2.1 Introduction A project charter documents the project’s scope, guiding principles, objectives, and roles and responsibilities. Creating and implementing a charter helps to optimize resources for the project and ensure the high quality of data and other outputs generated in the course of the project. For interdisciplinary and highly collaborative research (IHCR) a charter helps to create a shared understanding of the project and align diverse backgrounds and expectations. It can also act as an ethical guide to ensure proper credit and attribution and to resolve conflicts. Charters are meant to be living documents that evolve with the project and the team’s needs. They need to be written collaboratively and revised regularly. Drafting a project charter can facilitate important conversations early in the project to mitigate conflict or misunderstandings. It can also be a useful tool for onboarding new team members. The charter workflow below is organized into four broad sections: Brainstorm and Clarify, Work Together, Manage Data, Plan Outputs. After going through the steps of this workflow, the project team should have a good draft of the project charter to discuss and finalize. 2.2 Workflow Diagram 2.3 Detailed Steps 2.3.1 Brainstorm and Clarify Get together as a team and create a concise description of your project and its vision. Clarify the “why’s” and “what’s” of the project and make sure that the team has a shared understanding of the goals and scope of the project. Discuss assumptions and pre-existing conditions, e.g., funding and constraints, support from informal campus partners, and disciplinary differences. Discuss project continuity and sustainability. Will the project continue beyond its initial timeline? How will you deal with turnover of project members? What happens if a team member moves institutions? Suggested activities: Create an “elevator pitch” for your project. Identify the project’s strengths, weaknesses, and constraints. Discuss best case and worst case scenarios for project results. Create a team-generated glossary of terms. 2.3.2 Work Together Understand your team and what each part of the project depends on. Does the project include students, external consultants, volunteers? What are the roles of everyone on the project? Discuss expectations and modes of communication for your team, including any project management tools you will use, frequency of meetings, and expectations for response times, including communication outside of regular meetings. Establish the main principles that will guide the work of your team. Possible topics to cover include: Values (e.g., good faith and respect, openness and honesty) Governance (how decisions are made and how to resolve conflicts and disagreements) Recognizing diverse labor contributions Open and transparent research Diversity, equity, inclusion, &amp; access Suggested activities: Review available communication channels and resources. Discuss conflict resolution techniques. Discuss how to balance disciplinary perspectives with the project’s goals. 2.3.3 Manage Data Identify the data you already have, secondary data you plan to use, and data you need to collect. Discuss methods of data gathering and analysis, especially if methods come from different disciplines. Does the team have the necessary skills? Which tools will be needed? Does everyone in the team have access to those tools? Identify how data will be stored, organized, and shared. Discuss backup procedures. Establish rules for archiving or preservation of research data post-project, compliance with funding agencies, and embargo rules. Suggested activities: Create a data management plan. Develop a data catalog. Establish procedures for documenting metadata (describing your data). 2.3.4 Plan Outputs Discuss the intended audiences for this research and the impact of research on various fields on team members’ careers. Identify potential venues and forms of publications, e.g., papers, presentations, datasets, and other research outputs. Discuss how interdisciplinarity affects the choice of venues and publications. Sketch out a timeline with major milestones and deadlines. Be sure to include any funding or publication/conference deadlines. List major tasks towards the project deliverables. These might align with the research lifecycle and could include things like: acquire equipment, train students on data collection, etc. If possible, indicate responsible team members. These should be high level tasks. You may also discuss how you will adjust for unexpected delays. Develop rules for authorship and attribution. IHCR projects involve many actors at different levels. Make sure that all team members get credit for their work and identify which contributions merit author recognition on papers and presentation. Other considerations include decisions regarding authorship order, creating a project website with a comprehensive list of team members, and how disagreements will be resolved (especially, in the cases of a power imbalance). Suggested activities: Select a project management approach (e.g., a tracking spreadsheet, external tools such as Trello.com, etc.) Review contributor roles taxonomy (e.g., https://credit.niso.org/) and discuss how these roles apply to your project. 2.4 Charter Template Project title Project description (3-5 paragraphs) Elevator pitch (1 paragraph) Team members and their roles Project principles and values Infrastructure Communication tools Storage Analysis software Deliverables and due dates 2.5 Resources and Examples Indiana University Institute for Digital Arts &amp; Humanities Project Charter Template Center for Digital Humanities Princeton Project Charters Budak, N., Rustow, M., Koeser, R. S., Doroudian, G., Ermolaev, N., Luescher, S., &amp; Richman, R. (2020). CDH Project Charter – Princeton Geniza Project 2020-2021. Center for Digital Humanities at Princeton. https://doi.org/10.5281/zenodo.4081714 Ruecker, S., &amp; Radzikowska, M. (2008). The iterative design of a project charter for interdisciplinary research. Proceedings of the 7th ACM Conference on Designing Interactive Systems, 288–294. https://doi.org/10.1145/1394445.1394476 "],["Info-org.html", "Chapter 3 Organizing Information and Data for Interdisciplinary Projects 3.1 Introduction 3.2 Workflow Diagram 3.3 Detailed Description of Steps 3.4 Resources and Examples", " Chapter 3 Organizing Information and Data for Interdisciplinary Projects 3.1 Introduction Efficient information storage and organization is integral to interdisciplinary and highly collaborative research (IHCR). Even though working with IHCR data is in many ways similar to working with data that comes from disciplinary projects, there are some important differences: Many IHCR projects are grant-funded, so the need for efficient long-term management of data is more pressing considering that the funding may not be renewed. IHCR projects are characterized by data and methodological heterogeneity, i.e., the data collection and analysis techniques come from various domains that have varying rules and procedures. IHCR projects often use online platforms in addition to traditional scholarship venues (papers, conferences) to disseminate their work. IHCR projects are often “cross-institutional,” which makes storage and organization even more complex. Due to their shorter and more heterogeneous nature, IHCR projects require more coordination. A consistent approach to information storage and organization can help reduce the burden of coordination and allow the project to track and update resources as needed. The workflow below provides a set of recommended steps for organizing data and information. 3.2 Workflow Diagram 3.3 Detailed Description of Steps 3.3.1 Set goals The goals of data and information organization (which in most practical cases is synonymous with file organization) need to be aligned with the goals of the project. For research projects, the goals usually include collecting and analyzing data and delivering results in the form of papers and presentations. For IHCR projects, the goals also need to address their heterogeneous nature. In addition to the project goals, goal-setting for file organization includes understanding who will be using the file system. 3.3.2 Determine storage needs Determining storage needs includes determining the amount of storage that will be needed for the project, modes and rules of access, existing storage options, and the alignment between storage and computation. To determine project storage needs, consider the following: With whom the data will be shared regularly (e.g., collaborators at your institution, external collaborators)? What is the classification of your data (e.g., public, sensitive, confidential/restricted, HIPAA-compliant)? Many universities have classifications of their institutional data, which can be used as a guideline. What are your data safety procedures? (e.g., number of backup copies, backup locations, frequency of snapshots). How much data do you have and how fast will it grow? (e.g., under 1TB/year, over 1TB/year). Do you have special performance needs? (e.g., thousands of files, frequent transfer, large computational needs). How are you expecting to access the data? (e.g., from own computer, remotely from any computer). Who is responsible for the data storage and its maintenance? (e.g., yourself, another team member, your institution). What is your budget for data storage and management and where does it come from? (e.g., no budget, fixed amount from grant, ongoing support from institution). 3.3.3 Evaluate options and select the appropriate storage To evaluate storage options, the team may want to consult with institutional IT, the library, or a dedicated data management group at their respective institutions. 3.3.4 Organize files into folders and sub-folders Folder names and structure are highly dependent on the nature of the project. No files should be stored outside of folders, and each file is related to a category within an area. Start with a minimal hierarchy and add complexity only when certain that it will simplify daily work. Below is a suggested structure that can be adapted to many research projects: Administration Hiring Finances .. Grant materials Proposal Reports .. Instruments Questionnaires .. Data Collected Processed Analysis Publications and presentations Archive 3.3.5 Establish a file naming convention Define your file naming convention based on aspects that are important to the project. Use descriptive names that include a) a date, b) experiment conditions, c) location, d) researcher name/initials, e) version number. Decide whether to use underscore (_), hyphen (-), or camelCase consistently in file names between the aspects chosen. Include dates in the YYYYMMDD or YYYY-MM-DD format at the beginning of the name to keep files in chronological order date_content 3.3.6 Consider using tags and controlled vocabularies File names may be more useful if standardized information is added to them. For example, the filenames may include designations for sharing status, readiness, and so on. Data: [raw] [processed] [reshaped] [cleaned] Sharing: [noshare] [shared] [public] Readiness: [first draft] [revision] [final] Funding: [submitted] [rejected] [awarded] 3.3.7 Establish a system for version control Most commonly used method includes a combination of date, sequential numbering and researcher initials for version tracking. This method is prone to errors and requires a lot of effort in tagging / renaming each file. A better way for version control is to use the established version control systems such as Git, CVS, or SVN, however, they require investments in learning, training, and adoption. 3.3.8 Define access permissions Determine who on the team should be able to access and/or edit which files. Implement access controls as needed to ensure file integrity and/or comply with requirements for data that has been classified as sensitive, confidential, HIPAA-protected, etc. 3.3.9 Solicit input from team members Once the system has been in place for a reasonable amount of time, reach out to the team members and solicit their input on the following: What works well in the current system? What is not working with the current system? What is missing from the current system? Discuss and review the feedback and determine what changes can improve the system without adding too much burden to maintenance and implementation. 3.3.10 Maintain the system Schedule recurring maintenance time to review filing practices and move misplaced files. Review the existing files and folders and delete or archive what is not needed. Designate someone on your team as the go-to person for all file-organizing questions. Check in with your team periodically to see whether your organization system is working and adjust as necessary. 3.4 Resources and Examples Illinois Library File Naming Conventions How to Design File Management for a Company Gentzkow, M. &amp; Shapiro, J. M. (2014). Code and Data for the Social Sciences: A Practitioner’s Guide. pdf Data Storage Finder at Cornell University Store, share, and collaborate on institutional files at Indiana University File Transfer, Storage, and Infrastructure at University of Colorado Boulder File naming apps: Bulk Rename Utility (Windows), Renamer (Mac), PSRenamer "],["creating-and-maintaining-a-catalog-of-data-assets.html", "Chapter 4 Creating and Maintaining a Catalog of Data Assets 4.1 Introduction 4.2 Workflow Diagram 4.3 Implementation Steps 4.4 Appendices: Detailed Instructions", " Chapter 4 Creating and Maintaining a Catalog of Data Assets 4.1 Introduction Keeping track of information assets is a significant challenge in projects that conduct interdisciplinary and highly collaborative research (IHCR). In such projects, data and information may be spread across multiple storage systems and institutions; multiple contributors may be responsible for it at various stages of the project lifecycle. Commercial products such as Starfish provide cataloging and tracking functionality, but such products may be costly and have technical barriers for integration and maintenance. The workflow below provides a low-cost way for IHCR teams to create and maintain a catalog of their data assets. When used in combination with the overall project plan to guide teamwork and data management (see Section “Implementing a Charter..”), it will improve consistency and efficiency of IHCR data practices. While this workflow focuses on data, it can also be used for managing digital information assets more broadly. 4.2 Workflow Diagram 4.3 Implementation Steps 4.3.1 Assign Roles and Responsibilities The first step in tracking data assets is to designate the roles and responsibilities of project participants to decide who will be involved in data collection and management and how. Identify who will serve as the designated data manager and who will assume their responsibilities in the event the data manager is not available. Depending on the number of project participants, the responsibilities of the data manager can be assigned to one or more individuals. 4.3.2 Select Storage Once the project team is formed, members can review and decide on the data storage location and the directory structure for all project files and data assets. The choice of storage platform will also impact which of the data catalog options are available; solutions offered below are for local file storage, Google Drive, and the Microsoft 365 cloud suite (OneDrive and Sharepoint). See also the Section “Organizing Information..” 4.3.3 Establish Metadata Procedures The data manager (and other members of the team) will need to determine what information about data assets (metadata) is most helpful for data use and reuse. As a general rule, metadata is the who, what, when, where, why, and how of the research data. A basic set of metadata fields is described below in the section “Manual Creation of Data Catalog”, but there will likely be additional fields specific to the project that should be included for the most complete description of each data asset. 4.3.4 Setup and Create Data Catalog The data catalog can be set up and created via manual collection that uses a data cataloging form, automated inventory procedures (see “Appendices” for detailed instructions), or a combination of both methods. If using both the manual and automated collection methods, the data manager will need to reconcile the data catalog with the periodic inventories to maintain a full record of the project’s data assets. As part of the catalog setup, all project members need to be assigned access permissions. For example, the catalog should be viewable to any project member, but it should only be edited by the data manager(s) and data cataloger(s). As part of the catalog setup, all project members need to be assigned access permissions. For example, the catalog should be viewable to any project member, but it should only be edited by the data manager(s) and data cataloger(s). 4.3.5 Review and Update Regularly The data manager should periodically review the data catalog to ensure consistency and check that no information is missing. This includes confirming that file naming conventions are followed, that files are stored in the correct locations, and that there is no unnecessary duplication of files. If additional metadata fields were not added to the catalog by the workflows described in this document, they need to be added to the catalog manually. Periodic evaluation and update can be done in two ways: via a manual review of the data catalog or by running the automated inventory described in Appendix B. Reviews should be done at regular intervals to avoid a backlog of work and to catch and rectify potential errors in cataloging as early as possible instead of at the end of a project’s data collection. The reviews should be scheduled in weekly or monthly intervals depending on the pace of the project and its data collection processes. 4.4 Appendices: Detailed Instructions Appendix A. Manual Creation of Data The data catalog is a list of all data assets that are described consistently and uniformly using a set of attributes. Some attributes may be set to “optional” if they are not essential to each data asset. The catalog can be created and tracked manually via a data cataloging form using Google forms, Microsoft forms, Qualtrics, or any other surveying or spreadsheet software. It can also be created as a template spreadsheet where each attribute is a column and each data asset forms a row. The template below lists the suggested minimum information needed to describe each asset; it can be customized to add more metadata fields according to the needs of a project. Asset ID (required): Unique asset ID assigned automatically or by the data manager. This field needs to be entered directly into the catalog instead of through the cataloging form. Data Collector (required): Name(s) of individual(s) who collected the data. Contributor (if different from data collector, optional): Name of the individual who created the record. Date of Collection (required): Date(s) data was collected. Keyword(s): A list of project-specific descriptors that can help in search and organization. These tags can include experiment names, institutions, geographic locations, and so on. At the beginning the tags can be free-form, but later on it is recommended to standardize the tags and create a controlled vocabulary. Description (required): Short description of contents (what is the nature of this data, methods used in collection, any details of importance to the project). State (optional): Current state of the asset (e.g., to be created, created, cleaned, transformed, verified). The states are project-specific and can be determined by the team in advance. Storage Location (optional): Location of the asset being cataloged (e.g., central project location, individual member’s storage, etc.). Path (optional): Where or how to find this asset. Can be a full path (e.g., “C”:My DriveFilename.csv”) or a relative path (e.g., folder “Data files”). Notes (optional): Any notes about the item being cataloged that are useful to include in the catalog. Appendix B. Data Catalog via Automatic Inventory B1. Use Microsoft Excel to Create a Data Catalog of a Local Directory (Windows Machine Only) The following steps will create a file list of a specified directory (and any subdirectories) stored on a local machine. The file list will include file information metadata and the link to both the individual file and to the folder a file appears in. Open a new workbook in Excel, then navigate to “Data”-&gt; “Get Data”-&gt; “From File” -&gt;“From Folder” Choose the directory you want to index and click “OK” (older versions of Excel) or “Open” (newer versions). In the new dialogue box that opens, choose “Transform Data”. In the Power Query window that opens, the fields that will be created for each file are listed as columns. To remove a field before running the query, right click the heading title and choose “Remove”. Click the “Close and Load” button at the top left corner on the menu; the data will then populate in the sheet. Save the file according to the file naming convention determined by the data manager to save a static copy of the directory’s contents. To have snapshots of the catalog over time, create an inventory periodically and add a date to the file name. B2. Create a Data Catalog of Microsoft Online Document Storage (SharePoint) and Save It on a Local Machine (Windows Machine Only) Note: This workflow generates a web query file “query.iqy” that cannot be run or opened on a Mac as the internet query connection is not supported by Excel for Mac. As a workaround, you can run this on Excel for Windows, save the file as XLSX and then open it in Excel for Mac. You’ll be able to see the data but you won’t be able to refresh the data connection. Open SharePoint in a browser and log in as necessary. Navigate to the directory that needs to be indexed. Note: Organizations may have their SharePoint platforms configured differently. Navigate to “Documents” within the project’s Sharepoint page and then open the directory to be indexed. Click “Export to Excel”. This will generate and download an Internet Query File called “query.iqy” (the file should download into the default download folder for your browser). Navigate to your download folder and double-click on the ‘query.iqy’ file. Enable a connection to your online SharePoint platform when prompted (see image below). The file will be opened as an empty workbook initially. You will be asked to select how the data should be imported. Select “Table” to have the full folder contents listed (selecting “PivotTable Report” or “PivotChart” will aggregate and summarize the data rather than generate full folder contents). Next, under “Where do you want to put the data?”, select the location for the index (in a designated area of the existing worksheet, a new worksheet in the current file, or in a new workbook). The new file will be populated with the following information: Name: name of the file or folder with a link to its online version Modified: the date when the item was modified Modified by: name of the user who modified the item last Item type: “item” refers to files, “folder” refers to folders Path: the path within folder hierarchy (no link to the online content). Save the file according to the file naming convention determined by the data manager to save a static copy of the directory’s contents. To run the index again on the same directory, either run the downloaded iquery file again or follow the steps above to download a new iquery file. If you plan to use the same iquery file to index a directory each time, it is recommended that you rename the file with the name of the Sharepoint Site and Directory it indexes. By default, the data catalog generated by this query is arranged alphabetically by name with all folders going first and all items (files) going after all the folders. To view the index by directory, sort the rows alphabetically by the column “Path” (Home -&gt; Sort &amp; Filter - Custom Sort). To sort further within each folder, use “Custom Sort” and add columns to the sort parameters (e.g., name or modified). To view files only (without the folder names in the column “Name”), click on the down arrow to the right of the column “Item Type” to view options for that field, and then deselect “Folder”. To only display items from specific folder(s) for review, click on the down arrow to the right of the Path column, uncheck “Select All”, and then select only the desired folder(s): 4.4.1 B3. Create a Data Catalog in Google Drive using a Google App Script The following steps will create a recursive file listing of a specified directory (and any subdirectories) stored in Google Drive with metadata, the file path, and links to both the individual file and to the folder a file appears in. Create a new google sheet to store the file inventory in your project directory. To add the App Script to the document, go to “Extensions”-&gt; “Apps Script”. In the App Script that opens, remove any content that is there and paste the code block from the end of this section (if needed, the metadata fields to be collected can be changed and/or reordered in the last section of code). Give the script a meaningful name (e.g., “FileList”), then save the project by clicking on the diskette icon at the top. Click “Run”. Close the tab with the Apps Script and the tab with the google sheet, then reopen the sheet. You will now see a new menu option named “File List”. To run the inventory, click “File List” then choose “Create a file list from folder”. Enter the name of the directory whose contents you want to inventory in the dialogue box that opens. If asked to allow access, choose “Continue” then provide your credentials. (You may need to repeat the previous step after providing authorization). The script will then populate the sheet with a list of files from the specified directory with the metadata specified. You can run the script in multiple worksheets within the same GoogleSheet for different directories if needed, or run it for the full directory. Code for Apps Script //Function that creates the menu link in the sheet to run the inventory function onOpen() { var ss = SpreadsheetApp.getActiveSpreadsheet(); var searchMenuEntries = [{ name: &quot;Create a File List from Folder&quot;, functionName: &quot;start&quot; }]; ss.addMenu(&quot;File List&quot;, searchMenuEntries); } //Function that builds the inventory function start() { var sheet = SpreadsheetApp.getActiveSheet(); sheet.clear(); //Comment below enable it if you need one sheet //Builds the column headings sheet.appendRow([&quot;Name&quot;, &quot;Folder&quot;, &quot;Type&quot;, &quot;Creation Date&quot;, &quot;Size&quot;, &quot;URL&quot;,&quot;Folder URL&quot;, &quot;Path&quot; ]); //Makes the Input Box to specify which folder the inventory will run on var folderName = Browser.inputBox(&quot;Enter the Name of the Directory to be Inventoried (this will overwrite the current spreadsheet):&quot;); //Fetches the folders var folder = DriveApp.getFoldersByName(folderName); if (folder.hasNext()) { processFolder(folder, &#39;&#39;); } else { Browser.msgBox(&#39;Folder not found!&#39;); } //Returns the information for each file in the specified folder function processFolder(folder, path) { while (folder.hasNext()) { var f = folder.next(); var contents = f.getFiles(); var fName = f.getName(); addFilesToSheet(contents, f, path); var subFolder = f.getFolders(); processFolder(subFolder, path + &#39;/&#39; + fName); } } //Outputs the file inventory function addFilesToSheet(files, folder, path) { var data; var folderName = folder.getName(); while (files.hasNext()) { var file = files.next(); Logger.log(file.getName()); Logger.log(file.getParents()); //Specifies the fields returned in the inventory var fileName = file.getName(); sheet.appendRow([ fileName, folderName, file.getMimeType(), file.getDateCreated(), file.getSize()/1024, file.getUrl(), folder.getUrl(), path + &quot;/&quot; + folderName + &quot;/&quot; + fileName, ]); } } } 4.4.2 B4. Create a Data Catalog of Files Stored in a Shared Linux File Mounting System The following steps will create a catalog of files stored in a shared project space in a Linux File Mounting System, such as Lustre, using Terminal. Using Linux terminal, connect to the file system where your project data is stored (for this, you will need to be connected via ssh, not sftp). Navigate to the directory where your data is stored and create a directory named ‘Data Catalog’ to store catalog files. Run the following command, replacing ‘filename’ with the name you want for the Data Catalog and ‘date’ with the current date. Find “$PWD” -type f | xargs ls -l &gt; ~/DataCatalog/filename_date.csv This will create a .csv file with a list of each file with its path and its file size and save it in your home directory on the file system in the directory you’ve created named ‘DataCatalog’. "],["processing-collected-quantitative-data-for-easy-analysis-input.html", "Chapter 5 Processing Collected Quantitative Data for Easy Analysis Input 5.1 Introduction 5.2 Workflow Diagram 5.3 Detailed Steps 5.4 References", " Chapter 5 Processing Collected Quantitative Data for Easy Analysis Input 5.1 Introduction Within interdisciplinary or highly collaborative research groups, quantitative data collected by one subgroup may require analysis by another subgroup. In seeking efficiency and internal logic, the subgroup collecting the data may collect and record the data in a manner that does not facilitate subsequent data analysis. For example, data about agricultural production is commonly collected in an order related to its location in the field, e.g. moving from one agricultural plot to an adjacent agricultural plot. While collecting agricultural production data in this way is less time consuming, it does not commonly lead to a set of data amenable for immediate data analysis. Thus processing collected quantitative data from one subgroup such that data analysis is easier for another subgroup is commonly needed. Here we present general steps (including a diagram), guidelines, and references for automating processing of collected quantitative data from one subgroup such that data analysis by another subgroup is eased. Vital to creating a successful process to transform collected data for easy analysis input is obtaining appropriate context from the data collection and data analysis subgroups. To make this process modular and extensible for future data collection, good software engineering and documentation practices are also important. 5.2 Workflow Diagram 5.3 Detailed Steps These steps are written as if the person automating the processing of collected quantitative data for analysis input is not associated with either data collection or data analysis. If they are associated with either, steps for which they already have the appropriate content or context can be skipped. Obtain from the research group a representative sample of collected data, including information on how the data was collected and why it was collected that way (i.e., its metadata). “Representative” in this case means that the sample of collected data and metadata is sufficient to fully understand the data collection process and the nature and structure of the data. Ask the data collection subgroup about the number of records in the collected dataset to ensure the planned algorithm is appropriate for pre-processing this size of data. Confirm that the data collection process will be similar in the future; otherwise developing an automated process may not be as beneficial. Obtain from the research group a representative sample of processed data ready for input into the analysis application, including what analysis application and input data structure will be used. “Representative” here means that the sample of processed data, including metadata, is sufficient to fully understand the data processing procedure. Develop algorithm to pre-process and document preprocessing workflow for case study. Share diagram or example of algorithm with subgroup representatives for their review, editing and ultimate approval. With input from subgroup representatives, may determine that data collection process could/should change, or that preprocessing output should be altered. Could be an iterative process. Ask each subgroup representative what context is required for both sub-groups to understand how preprocessing data for analysis input is to be done (and also for the use of a wider community) Include this context in algorithm for later documentation. Recommend output of data for analysis input to be in open formats (e.g. csv). Develop script/code to execute agreed-upon algorithm. See if packages available from frictionlessdata or other already existing functions will support this pre-processing. Use programming/scripting language that is familiar to subgroup representatives. Script/program documents provenance of pre-processing workflow (possibly in a log file) for improved data re-use. Incorporate best practices of working with data (e.g., keeping raw data intact, using code commenting, assertions). Documentation for the code should include: - Information on how data was collected by that particular subgroup. - Context agreed upon by subgroup representatives to understand data processing. - Format of data after preprocessing, and its use in data analysis by that particular subgroup. Other software documentation information according to best practices (e.g., description of dependencies). See Data Curation Network primers for specific languages/applications. Demonstrate script/code to research group members, revise as necessary. Add appropriate open-source license to the code and its documentation. Place a copy of the script, code, and documentation in a publicly accessible location for further development. One should also be placed in an archival repository. 5.4 References Ten simple rules for making research software more robust ESIP software guidelines Data Curation Network primers Software Metadata Recommended Format (SMRF) Guide "],["preserving-data-from-digital-projects-published-online.html", "Chapter 6 Preserving Data from Digital Projects Published Online 6.1 Introduction 6.2 Workflow Diagram 6.3 Implementation Steps 6.4 Resources and Examples", " Chapter 6 Preserving Data from Digital Projects Published Online 6.1 Introduction Digital projects are research projects that are designed, implemented, and disseminated using digital technologies. These projects take many different forms, such as online exhibitions, digital archives, data visualizations, interactive maps, and multimedia publications. Such projects are common in the digital humanities, but researchers in the social and other sciences also rely on digital projects to disseminate their work online. Digital projects are often interdisciplinary and collaborative as they draw on methods and theories from various fields and involve collaboration between scholars, scientists, librarians, and technologists. Preserving data from digital scholarly projects ensures long-term access to the research and knowledge contributions behind those projects. A workflow for preserving the web component of a digital scholarly project can aim to preserve both the content and the user experience of a site or tool, or focus on preserving the content. The steps described below focus on the latter, i.e., on preserving the four essential components of a digital project: its content (data), code, processes, and user experiences [5]. These steps can also help incorporate data preservation efforts earlier in the data lifecycle. 6.2 Workflow Diagram 6.3 Implementation Steps 6.3.1 Establish Long-Term Storage To create an archive, use reliable storage that is designed for long-term preservation. Storage such as Google Drive, Sharepoint, etc. are useful for collaboration and the research phase of a project, but they are not a good solution for long-term storage, particularly, when all storage depends on one owner. To avoid future loss, data needs to be accessible by more than one person, backed up regularly and stored in multiple locations. Institutional repositories are an option for long-term storage. See also the Section “Organizing Information..” 6.3.2 Create a Data Lifecycle Management Plan A data management plan works best when it is created at the beginning of the project and then updated regularly. However, it is still useful to think through the management steps during the preservation phase. A digital project goes through the steps similar to other research projects, including the stages of planning, data collection, analysis, and dissemination. It also includes specific steps such as content creation, including digitization and transformation of data sources, technical development, including coding, hosting, and maintenance, and outreach, which includes efforts to widen the audiences that know and have access to the project [1]. Each of these steps includes data that will go into preservation, so the management plan will need to determine which elements, internal and external, need to be preserved. Overall, the plan should cover the following: Types of data and metadata to be preserved Access, privacy, and security specifications Cost of curation and preservation Backup, update, and versioning protocols Responsible parties Researchers and scholars may want to reach out to their institutional librarians and archivists to discuss migration and preservation options. The following questions can help in the discussion: How long will the archived data be stored? What maintenance will be required to keep archived data usable over time? Where will the archived data be hosted? Who will it be accessible to, and when? Depending on the repository, embargos may be possible if a team decides one is necessary. Once the management plan is in place, create a schedule for when it will be updated. 6.3.3 Map Data and Collaborations Creating a map of the assets that supported the online publication includes identifying source data, platform-specific data, and server (site) - related data and documenting their types, structure and processes that went into creating and transforming digital assets into their online form (e.g., ingesting, coding, conversions, encoding, etc.). List each subset of data (original data, platform-specific data, server and database files, dynamic/visual surrogates, and documentation) with their main components. This will provide an overview of the data archive for a project for team members, dataset users, etc. For file longevity, it is best to save this as either a .txt file or a .csv. If the online component of the project is hosted in a proprietary platform (such as ArcGIS for GIS data), include information on who has access to the account and where credentials are stored. Download all materials possible from the proprietary site and create non-proprietary surrogates that are close to proprietary forms (e.g., still images, screen recordings of the site/resources, data snapshots). Document all collaborators and contributors and create a short project description, including its purpose, components, funding, etc. (see also the Section “Implementing a Charter..”) To organize and document original data sources, choose formats that are open, widely used, and likely will be sustainable in the long-term. Avoid proprietary and deprecated formats. Include and document any code written for data extraction, cleaning, or structuring. If the transformations were performed manually or using point-and-click interfaces (e.g., Excel), document the steps taken during those transformations. To organize data that were transformed into platform-specific data (e.g., site maps, word clouds, animations, dynamic pages, etc.), capture and document the most important versions. If possible, export them into a static non-proprietary form. Depending on the platform, not everything can be downloaded directly. For example, for GIS projects using ArcGIS, feature layers can be opened in ArcGIS Pro and then saved as part of a project file, but will still be in a proprietary format. For visualizations or tools, include media showing the appearance of those features in the archival package. This includes videos of any dynamic resources, still photos of maps and other visualizations as image files and/or pdfs. For custom code built into the site, include files with all custom code for the site in the archival package, including documentation for that code that would allow an individual not associated with the original project to understand the code and be able to utilize it in a new version of the project if migration or disaster recovery becomes necessary. For server and database files, verify if a database export is available and use site documentation to preserve those. Include server settings and customizations that allowed to create a particular look of the project (e.g., color scheme, layout, font, etc.). Export and save assets (server files, database contents, code, files, documentation) in a logically structured form. 6.3.4 Document Content, Code, Processes, and User Experiences The preservation package should include documentation for all four components of the digital project - its content (data), code, processes, and user experiences. The latter will help to get a sense of how the site works without access to the full working copy. Depending on the resources, this could include recordings of the website or static screen shots. This will aid in recovery and re-implentation. Create “Read Me” documents that describe all the components and sufficient information for recreating the project [4]. Include documentation for building the site (including data ingest) and other web resources. Document the rights for different parts of the archived data and ensure there is a clear statement of any rights restrictions at the top level of the overall archive. Determine what training and education materials should accompany the archive (e.g., a how-to or a tutorial). Develop policies and procedures for managing the digital assets, including guidelines for access, storage, backup, migration, and preservation. Set a schedule for backups that matches with the update frequency of the online component (if data is added weekly, run backups at least once a month, if yearly once a year, etc.). Establish or finalize agreements that guide publication, migration, and licensing/copyright of web resources. Determine if the project falls within the scope of the institutional digital preservation policy (see this example from Indiana University). 6.4 Resources and Examples 1.Ithaka S+R: Life cycle of digital resources. 2.The Endings Project: Building sustainable digital humanities projects. 3.Visualizing objects, places, and spaces: A digital project handbook (see also Project Stages https://handbook.pubpub.org/project-stages) 4.Cornell University Research Data Management Service Group. A guide to writing a “readme” style metadata. 5.Rockwell, G., Day, S., Yu, J., &amp; Engel, M. (2014). Burying dead projects: Depositing the globalization compendium. Digital Humanities Quarterly, 8(2). "],["other-outputs.html", "Chapter 7 Other Outputs", " Chapter 7 Other Outputs Interdisciplinary and Highly Collaborative Research (IHCR) Data Primer, Data Curation Network Introductory Data Management Guide for IHCR (TBA) Kouper, I. (2022). Data curation in interdisciplinary and highly collaborative research. International Journal of Digital Curation, 17(1), Article 1. https://doi.org/10.2218/ijdc.v17i1.835 Tuna, E., Chapman, K., &amp; Kouper, I. (2022). Data Management Workflows in Interdisciplinary Highly Collaborative Research. PEARC’22 Practice and Experience in Advanced Research Computing, 1–3. doi: 10.1145/3491418.3535183 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
